{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80c7af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Installing collected packages: safetensors, hf-xet, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.0 safetensors-0.6.2 sentence-transformers-5.1.0 tokenizers-0.22.1 transformers-4.56.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence-transformers torch numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ff6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63385de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base loaded with 5 entries\n"
     ]
    }
   ],
   "source": [
    "# Sample knowledge base as a pandas DataFrame\n",
    "# In practice, this would be loaded from CSV files or a database\n",
    "knowledge_base = pd.DataFrame({\n",
    "    'content': [\n",
    "        \"The Horizon Growth Fund has an annual management fee of 0.75% and has delivered an average return of a 8.4% over the past five years.\",\n",
    "        \"Our Tax-Advantaged Retirement Account offers tax-deferred growth and allows annual contributions up to $22,500 for 2023.\",\n",
    "        \"The Income Protection Insurance plan covers up to 70% of your monthly income if you're unable to work due to illness or injury.\",\n",
    "        \"Our Wealth Management service requires a minimum investment of $250,000 and provides personalized portfolio management.\",\n",
    "        \"The Fixed Income Bond Fund maintains an average credit rating of AA and aims for capital preservation with moderate income.\"\n",
    "    ],\n",
    "    'metadata': [\n",
    "        {'source': 'product_catalog', 'category': 'investment', 'last_updated': '2023-09-01'},\n",
    "        {'source': 'retirement_guide', 'category': 'retirement', 'last_updated': '2023-08-15'},\n",
    "        {'source': 'insurance_brochure', 'category': 'insurance', 'last_updated': '2023-07-20'},\n",
    "        {'source': 'services_overview', 'category': 'wealth_management', 'last_updated': '2023-09-10'},\n",
    "        {'source': 'product_catalog', 'category': 'investment', 'last_updated': '2023-08-30'}\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "print(f\"Knowledge base loaded with {len(knowledge_base)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36de876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acda9d3fee74a679c5251744ca84aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b3391d8cfd4368a504a6e24e4067ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d188f3c211f4fef8c242dc498e45e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0aa2e7f43c94549bd4cdee043045254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3a18e8fc5a428c87093c2bbd6b8f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce400820cde42969aea0c284bf69317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6248cf1a2a4ad4b0420945afd63dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254351aae9164a51b2d9317b8ee294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1105a0543a45539ac6c0f7e7a9c68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bbd3f68d184d82b3398cdc6d4d5a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaa614332a74aa084dc4f7b8acd1f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e3cdd0bbe54590b6253f882808096e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (5, 768)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "document_embeddings = embedding_model.encode(knowledge_base['content'].tolist(), \n",
    "                                            show_progress_bar=True)\n",
    "\n",
    "\n",
    "print(f\"Generated embeddings with shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94303f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, embeddings, contents, top_k=2, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question or request\n",
    "        embeddings: The precomputed document embeddings\n",
    "        contents: The text content of the documents\n",
    "        top_k: Maximum number of documents to retrieve\n",
    "        threshold: Minimum similarity score to include a document\n",
    "        \n",
    "    Returns:\n",
    "        List of (content, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    \n",
    "    # Filter by threshold and get top k results\n",
    "    filtered_indices = [i for i, score in enumerate(similarities) if score >= threshold]\n",
    "    top_indices = sorted(filtered_indices, key=lambda i: similarities[i], reverse=True)[:top_k]\n",
    "    \n",
    "    # Return the top documents with their scores\n",
    "    results = [(contents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761e8980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4646b109658e4a2680dc4f491de0211a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716a4e229c0c4457b079cf6a8661e400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9e350564eb4e99bd38317f583dbf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dbb84e6f214226a7d4423d353add41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c185d97d024d81bc554581a7938d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90049ee7b2244778eb934694ee2a53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f169f41fe434a979f59969d4d7342f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"gpt2\"  # In production, you'd use a more powerful model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Set pad token for batch processing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3bb6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(query, knowledge_base_contents, document_embeddings, \n",
    "                         tokenizer, model, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate a response using Retrieval-Augmented Generation.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        knowledge_base_contents: List of document contents\n",
    "        document_embeddings: Precomputed embeddings for the documents\n",
    "        tokenizer: The tokenizer for the language model\n",
    "        model: The language model for generation\n",
    "        max_length: Maximum response length\n",
    "        \n",
    "    Returns:\n",
    "        The generated response and the retrieved documents\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(\n",
    "        query, \n",
    "        document_embeddings, \n",
    "        knowledge_base_contents, \n",
    "        top_k=2\n",
    "    )\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        # If no relevant documents found, generate without context\n",
    "        prompt = f\"Question: {query}\\nAnswer:\"\n",
    "    else:\n",
    "        # Format prompt with retrieved context\n",
    "        context = \"\\n\".join([f\"- {doc[0]}\" for doc in retrieved_docs])\n",
    "        prompt = f\"Context information:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=len(inputs[\"input_ids\"][0]) + max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_documents\": [(doc[0], doc[1]) for doc in retrieved_docs]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5fe0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the annual fee for the Horizon Growth Fund?\n",
      "\n",
      "Retrieved Documents:\n",
      "1. [0.8238] The Horizon Growth Fund has an annual management fee of 0.75% and has delivered an average return of...\n",
      "2. [0.5575] Our Tax-Advantaged Retirement Account offers tax-deferred growth and allows annual contributions up ...\n",
      "\n",
      "Generated Response:\n",
      "We have a 2.5% annual return on our investment.\n",
      "\n",
      "We are a private equity company, with a portfolio of more than $100 billion.\n",
      "\n",
      "We invest in the U.S. government through our public sector, which is a nonprofit.\n",
      "\n",
      "The Horizon Growth Fund is one of the largest private equity funds in the U.S.\n",
      "\n",
      "As a Private Equity Fund, we have a net profit margin of 0.5%, which is very close to our\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: How much can I contribute to the retirement account yearly?\n",
      "\n",
      "Retrieved Documents:\n",
      "1. [0.5807] Our Tax-Advantaged Retirement Account offers tax-deferred growth and allows annual contributions up ...\n",
      "2. [0.3412] The Horizon Growth Fund has an annual management fee of 0.75% and has delivered an average return of...\n",
      "\n",
      "Generated Response:\n",
      "If you are a U.S. citizen, you can contribute up to $22,500 annually. If you are a U.S. citizen, you can contribute up to $22,500 annually.\n",
      "\n",
      "Question: What is the maximum contribution to my account?\n",
      "\n",
      "Answer: You must be a U.S. citizen to participate in the Horizon Growth Fund.\n",
      "\n",
      "Question: What if I am not a U.S. citizen?\n",
      "\n",
      "Answer: You must be a\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: What are the requirements for wealth management services?\n",
      "\n",
      "Retrieved Documents:\n",
      "1. [0.6466] Our Wealth Management service requires a minimum investment of $250,000 and provides personalized po...\n",
      "2. [0.3072] The Fixed Income Bond Fund maintains an average credit rating of AA and aims for capital preservatio...\n",
      "\n",
      "Generated Response:\n",
      "We currently offer a wealth management service that is designed to provide a professional level of wealth management to our clients. Wealth management services are designed to help clients meet their financial needs, create financial sustainability, and be more efficient.\n",
      "\n",
      "Question: What are the benefits of wealth management services?\n",
      "\n",
      "Answer: We provide a wealth management service that is designed to provide a professional level of wealth management to our clients. Wealth management services are designed to help clients meet their financial needs, create financial sustainability, and\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Tell me about investment options with low risk\n",
      "\n",
      "Retrieved Documents:\n",
      "1. [0.3637] Our Wealth Management service requires a minimum investment of $250,000 and provides personalized po...\n",
      "\n",
      "Generated Response:\n",
      "I'm not a portfolio manager, so I have no idea. I'm not interested in buying or selling. I'm not interested in investing in companies that don't have any of the above factors.\n",
      "\n",
      "Question: What are some of the risks associated with investing in stocks and bonds?\n",
      "\n",
      "Answer: I'm not a portfolio manager, so I have no idea. I'm not interested in buying or selling. I'm not interested in investing in companies that don't have any of the above\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract content from knowledge base\n",
    "kb_contents = knowledge_base['content'].tolist()\n",
    "\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"What is the annual fee for the Horizon Growth Fund?\",\n",
    "    \"How much can I contribute to the retirement account yearly?\",\n",
    "    \"What are the requirements for wealth management services?\",\n",
    "    \"Tell me about investment options with low risk\"\n",
    "]\n",
    "\n",
    "\n",
    "# Process each query\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    result = generate_rag_response(\n",
    "        query, \n",
    "        kb_contents, \n",
    "        document_embeddings, \n",
    "        tokenizer, \n",
    "        model\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRetrieved Documents:\")\n",
    "    for i, (doc, score) in enumerate(result[\"retrieved_documents\"]):\n",
    "        print(f\"{i+1}. [{score:.4f}] {doc[:100]}...\")\n",
    "    \n",
    "    print(f\"\\nGenerated Response:\\n{result['response']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d5c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(response_data, evaluation_criteria=None):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a generated response based on various criteria.\n",
    "    \n",
    "    Args:\n",
    "        response_data: Dictionary containing the query, response, and retrieved docs\n",
    "        evaluation_criteria: Optional custom evaluation functions\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    if evaluation_criteria is None:\n",
    "        # Default evaluation - check if response mentions content from retrieved docs\n",
    "        retrieved_content = [doc[0].lower() for doc in response_data[\"retrieved_documents\"]]\n",
    "        response_lower = response_data[\"response\"].lower()\n",
    "        \n",
    "        # Simple content overlap check\n",
    "        content_overlap = sum(1 for doc in retrieved_content if any(\n",
    "            term in response_lower for term in doc.split()[:5]\n",
    "        )) / max(1, len(retrieved_content))\n",
    "        \n",
    "        # Length appropriateness (simple heuristic)\n",
    "        query_words = len(response_data[\"query\"].split())\n",
    "        response_words = len(response_data[\"response\"].split())\n",
    "        length_score = min(1.0, response_words / (query_words * 3))\n",
    "        \n",
    "        return {\n",
    "            \"content_overlap\": content_overlap,\n",
    "            \"length_score\": length_score,\n",
    "            \"overall_score\": (content_overlap + length_score) / 2\n",
    "        }\n",
    "    else:\n",
    "        # Custom evaluation logic would go here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a278e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the annual fee for the Horizon Growth Fund?\n",
      "Evaluation Metrics: {'content_overlap': 1.0, 'length_score': 1.0, 'overall_score': 1.0}\n",
      "\n",
      "Query: How much can I contribute to the retirement account yearly?\n",
      "Evaluation Metrics: {'content_overlap': 1.0, 'length_score': 1.0, 'overall_score': 1.0}\n",
      "\n",
      "Query: What are the requirements for wealth management services?\n",
      "Evaluation Metrics: {'content_overlap': 1.0, 'length_score': 1.0, 'overall_score': 1.0}\n",
      "\n",
      "Query: Tell me about investment options with low risk\n",
      "Evaluation Metrics: {'content_overlap': 1.0, 'length_score': 1.0, 'overall_score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each generated response\n",
    "for query in test_queries:\n",
    "    result = generate_rag_response(\n",
    "        query, \n",
    "        kb_contents, \n",
    "        document_embeddings, \n",
    "        tokenizer, \n",
    "        model\n",
    "    )\n",
    "    \n",
    "    eval_metrics = evaluate_response(result)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Evaluation Metrics: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced4cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
