{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fed693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Requirement already satisfied: surprise in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from surprise) (1.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from scikit-surprise->surprise) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from scikit-surprise->surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from scikit-surprise->surprise) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the Surprise library\n",
    "!pip uninstall -y numpy\n",
    "!pip install numpy==1.26.4\n",
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277054e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import numpy as np\n",
    "from surprise import Dataset  # For loading and handling datasets\n",
    "from surprise import Reader   # For parsing custom datasets\n",
    "from surprise import SVD      # Singular Value Decomposition algorithm\n",
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore  # K-Nearest Neighbors algorithms\n",
    "from surprise import NMF      # Non-negative Matrix Factorization algorithm\n",
    "from surprise import BaselineOnly  # Basic algorithm using baselines\n",
    "from surprise.model_selection import train_test_split  # For splitting data\n",
    "from surprise.model_selection import cross_validate    # For cross-validation\n",
    "from surprise.model_selection import GridSearchCV      # For hyperparameter tuning\n",
    "from surprise import accuracy  # For computing prediction accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2972a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the built-in MovieLens dataset\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ac5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets (75% training, 25% testing)\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d08c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "\n",
      "Algorithm Comparison:\n",
      "------------------------------------------------------------\n",
      "Algorithm                      RMSE            MAE            \n",
      "------------------------------------------------------------\n",
      "SVD                            0.9352          0.7375         \n",
      "KNNBasic (User-based)          0.9793          0.7735         \n",
      "KNNBasic (Item-based)          0.9735          0.7690         \n",
      "KNNWithMeans (User-based)      0.9501          0.7483         \n",
      "NMF                            0.9618          0.7558         \n",
      "BaselineOnly                   0.9439          0.7482         \n"
     ]
    }
   ],
   "source": [
    "# Define a list of algorithms to compare\n",
    "algorithms = [\n",
    "    SVD(),\n",
    "    KNNBasic(sim_options={'user_based': True}),  # User-based collaborative filtering\n",
    "    KNNBasic(sim_options={'user_based': False}), # Item-based collaborative filtering\n",
    "    KNNWithMeans(sim_options={'user_based': True}),\n",
    "    NMF(),\n",
    "    BaselineOnly()\n",
    "]\n",
    "\n",
    "# Evaluate each algorithm using cross-validation\n",
    "results = {}\n",
    "for algo in algorithms:\n",
    "    algo_name = algo.__class__.__name__\n",
    "    sim_option = ''\n",
    "    \n",
    "    # Add user/item based info for KNN algorithms\n",
    "    if algo_name.startswith('KNN'):\n",
    "        user_based = algo.sim_options.get('user_based', True)\n",
    "        sim_option = 'User-based' if user_based else 'Item-based'\n",
    "        algo_name = f\"{algo_name} ({sim_option})\"\n",
    "    \n",
    "    # Run 5-fold cross-validation\n",
    "    cv_results = cross_validate(algo, data, measures=['RMSE', 'MAE'], \n",
    "                               cv=5, verbose=False)\n",
    "    \n",
    "    # Store results\n",
    "    results[algo_name] = {\n",
    "        'RMSE': cv_results['test_rmse'].mean(),\n",
    "        'MAE': cv_results['test_mae'].mean()\n",
    "    }\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nAlgorithm Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Algorithm':<30} {'RMSE':<15} {'MAE':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for algo_name, metrics in results.items():\n",
    "    print(f\"{algo_name:<30} {metrics['RMSE']:<15.4f} {metrics['MAE']:<15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c83877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RMSE Parameters:\n",
      "{'n_factors': 100, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1, 'random_state': 42}\n",
      "Best RMSE Score: 0.9119\n",
      "\n",
      "Best MAE Parameters:\n",
      "{'n_factors': 100, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1, 'random_state': 42}\n",
      "Best MAE Score: 0.7212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1082be330>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid for SVD\n",
    "param_grid = {\n",
    "   'n_factors': [50, 100, 150],\n",
    "   'n_epochs': [10, 20, 30],\n",
    "   'lr_all': [0.002, 0.005, 0.01],\n",
    "   'reg_all': [0.02, 0.1, 0.5],\n",
    "   'random_state': [42]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "gs.fit(data)\n",
    "\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest RMSE Parameters:\")\n",
    "print(gs.best_params['rmse'])\n",
    "print(f\"Best RMSE Score: {gs.best_score['rmse']:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nBest MAE Parameters:\")\n",
    "print(gs.best_params['mae'])\n",
    "print(f\"Best MAE Score: {gs.best_score['mae']:.4f}\")\n",
    "\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_algo = SVD(**gs.best_params['mae'])\n",
    "best_algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62e6ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 391, Item: 591, Actual Rating: 4.00, Predicted Rating: 3.43, Error: 0.57\n",
      "User: 181, Item: 1291, Actual Rating: 1.00, Predicted Rating: 1.52, Error: -0.52\n",
      "User: 637, Item: 268, Actual Rating: 2.00, Predicted Rating: 2.77, Error: -0.77\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testset\n",
    "predictions = best_algo.test(testset)\n",
    "\n",
    "# Look at the first few predictions\n",
    "for pred in predictions[:3]:\n",
    "    print(f\"User: {pred.uid}, Item: {pred.iid}, \"\n",
    "          f\"Actual Rating: {pred.r_ui:.2f}, Predicted Rating: {pred.est:.2f}, \"\n",
    "          f\"Error: {pred.r_ui - pred.est:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345fe05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for user 196:\n",
      "Movie ID: 408, Predicted Rating: 4.47\n",
      "Movie ID: 64, Predicted Rating: 4.43\n",
      "Movie ID: 318, Predicted Rating: 4.43\n",
      "Movie ID: 483, Predicted Rating: 4.42\n",
      "Movie ID: 169, Predicted Rating: 4.42\n"
     ]
    }
   ],
   "source": [
    "# Create a full training set that includes all users\n",
    "full_trainset = data.build_full_trainset()\n",
    "\n",
    "# Fit model to all users and data\n",
    "best_algo.fit(full_trainset)\n",
    "\n",
    "def get_top_n_recommendations(algo, data, user_id, n=10):\n",
    "    \"\"\"\n",
    "    Generate top-N recommendations for a specific user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    algo : surprise.prediction_algorithms\n",
    "        Trained algorithm\n",
    "    data : surprise.Trainset\n",
    "        The full training dataset\n",
    "    user_id : str\n",
    "        The user ID for whom to generate recommendations\n",
    "    n : int, default=10\n",
    "        Number of recommendations to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples\n",
    "        (item_id, predicted_rating) sorted by predicted rating in descending order\n",
    "    \"\"\"\n",
    "    # Get a list of all items\n",
    "    all_item_ids = data.all_items()\n",
    "    \n",
    "    # Convert raw user ID to inner ID used by the trainset\n",
    "    try:\n",
    "        inner_user_id = data.to_inner_uid(user_id)\n",
    "    except ValueError:\n",
    "        print(f\"User {user_id} doesn't exist in the training set\")\n",
    "        return []\n",
    "    \n",
    "    # Get items rated by this user\n",
    "    user_items = [j for (j, _) in data.ur[inner_user_id]]\n",
    "    \n",
    "    # Items not rated by the user\n",
    "    unrated_items = [item_id for item_id in all_item_ids if item_id not in user_items]\n",
    "    \n",
    "    # Predict ratings for unrated items\n",
    "    predictions = []\n",
    "    for item_id in unrated_items:\n",
    "        # Convert inner item ID back to raw ID for prediction\n",
    "        raw_item_id = data.to_raw_iid(item_id)\n",
    "        # Get prediction\n",
    "        pred = algo.predict(user_id, raw_item_id)\n",
    "        predictions.append((raw_item_id, pred.est))\n",
    "    \n",
    "    # Sort predictions by estimated rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top n recommendations\n",
    "    return predictions[:n]\n",
    "\n",
    "# Example usage\n",
    "user_id = '196'  # Choose a user ID from the dataset\n",
    "top_recommendations = get_top_n_recommendations(algo, trainset, user_id, n=5)\n",
    "\n",
    "print(f\"Top 5 movie recommendations for user {user_id}:\")\n",
    "for movie_id, predicted_rating in top_recommendations:\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {predicted_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573d615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
