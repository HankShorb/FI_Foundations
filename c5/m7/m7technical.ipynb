{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165cf821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (72.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "\u001b[33m  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813798 sha256=cd39913f38a1f65c0df3f5c2f361c4596e63b0955dd19be6e31866d83e72a935\n",
      "  Stored in directory: /Users/hank/Library/Caches/pip/wheels/31/9f/68/f89fb34ccd886909be7d0e390eaaf97f21efdf540c0ee8dbcd\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, mpmath, sympy, pyspark, fsspec, filelock, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.19.1 fsspec-2025.9.0 mpmath-1.3.0 py4j-0.10.9.9 pyspark-4.0.1 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark torch torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from keras import layers, Input\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706a7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Normalize California Housing Data\n",
    "housing = fetch_california_housing()\n",
    "df_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df_housing['target'] = housing.target  # Apply log transformation\n",
    "\n",
    "X = df_housing[housing.feature_names].values\n",
    "y = df_housing['target'].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682f2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001be40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP architecture to match TensorFlow\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        # First layer with BatchNorm\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Second layer with ReLU and then Dropout (matching TensorFlow placement)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)  # Moved dropout after ReLU to match TensorFlow\n",
    "        )\n",
    "        # Output layer\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "pt_model_regression = MLPRegressor(X_train.shape[1])\n",
    "\n",
    "# Use Adam optimizer with default settings to match TensorFlow\n",
    "# TensorFlow default Adam: lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7\n",
    "optimizer = optim.Adam(\n",
    "    pt_model_regression.parameters(),\n",
    "    lr=0.001,  # Default TensorFlow learning rate\n",
    "    betas=(0.9, 0.999),  # Default betas\n",
    "    eps=1e-8  # PyTorch default epsilon (slightly different from \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9947ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.9675\n",
      "Epoch [11/50], Loss: 0.4346\n",
      "Epoch [21/50], Loss: 0.3979\n",
      "Epoch [31/50], Loss: 0.3803\n",
      "Epoch [41/50], Loss: 0.3647\n",
      "Epoch [50/50], Loss: 0.3596\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create DataLoader for mini-batch training\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Match TF batch size=32\n",
    "\n",
    "# Training loop with mini-batches (50 epochs to match TensorFlow)\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Mini-batch training\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = pt_model_regression(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics (similar to TensorFlow verbose=1)\n",
    "    if epoch % 10 == 0 or epoch == num_epochs-1:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e1f56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.4928\n",
      "Test Mean Absolute Error: 0.4592\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "pt_model_regression.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = pt_model_regression(X_test_t)\n",
    "    mse_loss = criterion(predictions, y_test_t)\n",
    "    mae_loss = nn.L1Loss()(predictions, y_test_t)  # Calculate MAE to match TensorFlow metrics\n",
    "\n",
    "print(f'Test Mean Squared Error: {mse_loss.item():.4f}')\n",
    "print(f'Test Mean Absolute Error: {mae_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7f8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted House Price: 3.82\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with scaled input (matching TensorFlow's approach)\n",
    "sample_input_np = np.array([[8.0, 41.0, 6.0, 1.0, 950.0, 4.0, 37.0, -122.0]])\n",
    "sample_input_scaled = scaler.transform(sample_input_np)  # Apply scaling\n",
    "sample_input_t = torch.tensor(sample_input_scaled, dtype=torch.float32)\n",
    "\n",
    "pt_model_regression.eval()\n",
    "with torch.no_grad():\n",
    "    pt_prediction = pt_model_regression(sample_input_t).item()\n",
    "\n",
    "print(f'Predicted House Price: {pt_prediction:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faf74b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,945</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,945\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define MLP architecture\n",
    "tf_model_regression = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tf_model_regression.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Display model summary\n",
    "tf_model_regression.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6bc4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 613us/step - loss: 1.0765 - mae: 0.7598 - val_loss: 0.5155 - val_mae: 0.5021\n",
      "Epoch 2/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410us/step - loss: 0.6246 - mae: 0.5776 - val_loss: 0.6097 - val_mae: 0.5396\n",
      "Epoch 3/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step - loss: 0.5442 - mae: 0.5321 - val_loss: 0.5629 - val_mae: 0.5232\n",
      "Epoch 4/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433us/step - loss: 0.5055 - mae: 0.5107 - val_loss: 0.4253 - val_mae: 0.4455\n",
      "Epoch 5/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - loss: 0.4790 - mae: 0.4931 - val_loss: 0.4045 - val_mae: 0.4534\n",
      "Epoch 6/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - loss: 0.4512 - mae: 0.4800 - val_loss: 0.3795 - val_mae: 0.4233\n",
      "Epoch 7/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403us/step - loss: 0.4500 - mae: 0.4761 - val_loss: 0.3927 - val_mae: 0.4314\n",
      "Epoch 8/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - loss: 0.4336 - mae: 0.4670 - val_loss: 0.4150 - val_mae: 0.4442\n",
      "Epoch 9/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439us/step - loss: 0.4150 - mae: 0.4565 - val_loss: 0.4295 - val_mae: 0.4651\n",
      "Epoch 10/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402us/step - loss: 0.3962 - mae: 0.4452 - val_loss: 0.4237 - val_mae: 0.4626\n",
      "Epoch 11/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 0.3982 - mae: 0.4457 - val_loss: 0.4324 - val_mae: 0.4661\n",
      "Epoch 12/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 0.3906 - mae: 0.4453 - val_loss: 0.3569 - val_mae: 0.4245\n",
      "Epoch 13/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.3867 - mae: 0.4412 - val_loss: 0.3339 - val_mae: 0.4139\n",
      "Epoch 14/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.3786 - mae: 0.4353 - val_loss: 0.3664 - val_mae: 0.4354\n",
      "Epoch 15/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430us/step - loss: 0.3730 - mae: 0.4350 - val_loss: 0.3795 - val_mae: 0.4501\n",
      "Epoch 16/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427us/step - loss: 0.3728 - mae: 0.4351 - val_loss: 0.3713 - val_mae: 0.4481\n",
      "Epoch 17/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.3731 - mae: 0.4347 - val_loss: 0.4096 - val_mae: 0.4677\n",
      "Epoch 18/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.3589 - mae: 0.4238 - val_loss: 0.3884 - val_mae: 0.4562\n",
      "Epoch 19/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.3639 - mae: 0.4284 - val_loss: 0.3763 - val_mae: 0.4465\n",
      "Epoch 20/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.3710 - mae: 0.4320 - val_loss: 0.3211 - val_mae: 0.4045\n",
      "Epoch 21/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step - loss: 0.3635 - mae: 0.4287 - val_loss: 0.3684 - val_mae: 0.4476\n",
      "Epoch 22/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.3604 - mae: 0.4279 - val_loss: 0.4115 - val_mae: 0.4660\n",
      "Epoch 23/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.3688 - mae: 0.4311 - val_loss: 0.3183 - val_mae: 0.4009\n",
      "Epoch 24/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402us/step - loss: 0.3584 - mae: 0.4269 - val_loss: 0.3597 - val_mae: 0.4358\n",
      "Epoch 25/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.3567 - mae: 0.4245 - val_loss: 0.3785 - val_mae: 0.4488\n",
      "Epoch 26/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.3506 - mae: 0.4228 - val_loss: 0.3396 - val_mae: 0.4266\n",
      "Epoch 27/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3525 - mae: 0.4219 - val_loss: 0.3838 - val_mae: 0.4538\n",
      "Epoch 28/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3578 - mae: 0.4249 - val_loss: 0.3413 - val_mae: 0.4296\n",
      "Epoch 29/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.3550 - mae: 0.4235 - val_loss: 0.3160 - val_mae: 0.4120\n",
      "Epoch 30/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.3526 - mae: 0.4226 - val_loss: 0.3250 - val_mae: 0.4053\n",
      "Epoch 31/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - loss: 0.3475 - mae: 0.4194 - val_loss: 0.3580 - val_mae: 0.4362\n",
      "Epoch 32/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.3484 - mae: 0.4215 - val_loss: 0.3871 - val_mae: 0.4630\n",
      "Epoch 33/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.3464 - mae: 0.4198 - val_loss: 0.3663 - val_mae: 0.4347\n",
      "Epoch 34/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - loss: 0.3467 - mae: 0.4200 - val_loss: 0.3486 - val_mae: 0.4329\n",
      "Epoch 35/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3460 - mae: 0.4186 - val_loss: 0.3511 - val_mae: 0.4323\n",
      "Epoch 36/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3431 - mae: 0.4184 - val_loss: 0.3655 - val_mae: 0.4446\n",
      "Epoch 37/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - loss: 0.3461 - mae: 0.4185 - val_loss: 0.3825 - val_mae: 0.4499\n",
      "Epoch 38/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3461 - mae: 0.4187 - val_loss: 0.3713 - val_mae: 0.4482\n",
      "Epoch 39/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.3398 - mae: 0.4156 - val_loss: 0.3393 - val_mae: 0.4340\n",
      "Epoch 40/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.3460 - mae: 0.4187 - val_loss: 0.3432 - val_mae: 0.4213\n",
      "Epoch 41/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.3414 - mae: 0.4164 - val_loss: 0.3347 - val_mae: 0.4278\n",
      "Epoch 42/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.3452 - mae: 0.4174 - val_loss: 0.3312 - val_mae: 0.4262\n",
      "Epoch 43/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.3410 - mae: 0.4149 - val_loss: 0.3473 - val_mae: 0.4263\n",
      "Epoch 44/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.3379 - mae: 0.4149 - val_loss: 0.3112 - val_mae: 0.3984\n",
      "Epoch 45/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.3451 - mae: 0.4176 - val_loss: 0.3388 - val_mae: 0.4173\n",
      "Epoch 46/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - loss: 0.3376 - mae: 0.4148 - val_loss: 0.3556 - val_mae: 0.4238\n",
      "Epoch 47/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3399 - mae: 0.4140 - val_loss: 0.3333 - val_mae: 0.4133\n",
      "Epoch 48/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.3302 - mae: 0.4104 - val_loss: 0.3154 - val_mae: 0.4001\n",
      "Epoch 49/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - loss: 0.3306 - mae: 0.4102 - val_loss: 0.3110 - val_mae: 0.3977\n",
      "Epoch 50/50\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3346 - mae: 0.4122 - val_loss: 0.3085 - val_mae: 0.4031\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "tf_history_regression = tf_model_regression.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1041f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.3085 - mae: 0.4031\n",
      "Test Mean Absolute Error: 0.4031\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loss, mae = tf_model_regression.evaluate(X_test, y_test)\n",
    "print(f\"Test Mean Absolute Error: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b00c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
