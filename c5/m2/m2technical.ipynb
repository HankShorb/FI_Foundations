{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0196962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/anaconda3/envs/ml-env/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Using cached wrapt-1.17.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "Using cached wrapt-1.17.3-cp312-cp312-macosx_11_0_arm64.whl (39 kB)\n",
      "Installing collected packages: wrapt, scipy, smart-open, gensim\n",
      "\u001b[2K  Attempting uninstall: scipy\n",
      "\u001b[2K    Found existing installation: scipy 1.16.0\n",
      "\u001b[2K    Uninstalling scipy-1.16.0:\n",
      "\u001b[2K      Successfully uninstalled scipy-1.16.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [gensim]2m3/4\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.3.1 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a4e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d798429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cannot login to my account after password reset</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The website is loading very slowly on my browser</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The app crashes whenever I try to upload photos</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Videos are not playing properly on my device</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The checkout process gave me an error</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                       ticket_text department\n",
       "0          1   Cannot login to my account after password reset  technical\n",
       "1          2  The website is loading very slowly on my browser  technical\n",
       "2          3   The app crashes whenever I try to upload photos  technical\n",
       "3          4      Videos are not playing properly on my device  technical\n",
       "4          5             The checkout process gave me an error  technical"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('customer_support_ticket.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e135a8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (90, 3)\n",
      "\n",
      "Class distribution:\n",
      "department\n",
      "technical    30\n",
      "account      30\n",
      "billing      30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample tickets per department:\n",
      "\n",
      "TECHNICAL: Cannot login to my account after password reset\n",
      "\n",
      "ACCOUNT: How do I update my shipping address for my order?\n",
      "\n",
      "BILLING: My payment was charged twice for one order\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['department'].value_counts())\n",
    "print(\"\\nSample tickets per department:\")\n",
    "for dept in df['department'].unique():\n",
    "    sample_idx = df[df['department'] == dept].index[0]\n",
    "    print(f\"\\n{dept.upper()}: {df.loc[sample_idx, 'ticket_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8124204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The website is loading very slowly on my browser\n",
      "Cleaned: the website is loading very slowly on my browser\n",
      "Lemmatized: website load slowly browser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hank/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/hank/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hank/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Lemmatizer relies of part of speech to help\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "  '''\n",
    "  Translate nltk POS to wordnet tags\n",
    "  '''\n",
    "  if treebank_tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "      return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "  else:\n",
    "      return wordnet.NOUN\n",
    "\n",
    "\n",
    "def basic_preprocess(text):\n",
    "   \"\"\"Basic preprocessing function for text.\"\"\"\n",
    "   # Convert to lowercase\n",
    "   text = text.lower()\n",
    "  \n",
    "   # Remove special characters and numbers\n",
    "   text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "  \n",
    "   # Return cleaned text\n",
    "   return text\n",
    "\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "   \"\"\"Advanced preprocessing with tokenization, stopword removal, and lemmatization.\"\"\"\n",
    "   # Basic cleaning\n",
    "   text = basic_preprocess(text)\n",
    "  \n",
    "   # Tokenize\n",
    "   tokens = nltk.word_tokenize(text)\n",
    "  \n",
    "   # Tag with pos\n",
    "   tokens_tagged = pos_tag(tokens)\n",
    "   pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "  \n",
    "   # Remove stopwords and lemmatize\n",
    "   cleaned_tokens = [lemmatizer.lemmatize(token[0], token[1]) for token in pos_tokens if token[0] not in stop_words and len(token[0]) > 1]\n",
    "  \n",
    "   # Return cleaned tokens\n",
    "   return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['cleaned_text'] = df['ticket_text'].apply(basic_preprocess)\n",
    "df['lemmatized_text'] = df['ticket_text'].apply(advanced_preprocess)\n",
    "df['tokens'] = df['lemmatized_text'].apply(lambda x: x.split())\n",
    "\n",
    "\n",
    "# Show the preprocessing results for a sample ticket\n",
    "sample_idx = 1\n",
    "print(f\"Original: {df.loc[sample_idx, 'ticket_text']}\")\n",
    "print(f\"Cleaned: {df.loc[sample_idx, 'cleaned_text']}\")\n",
    "print(f\"Lemmatized: {df.loc[sample_idx, 'lemmatized_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d66364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 63\n",
      "Testing set size: 27\n",
      "Class distribution in training set: \n",
      "department\n",
      "technical    21\n",
      "account      21\n",
      "billing      21\n",
      "Name: count, dtype: int64\n",
      "Class distribution in testing set: \n",
      "department\n",
      "account      9\n",
      "technical    9\n",
      "billing      9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   df['ticket_text'],\n",
    "   df['department'],\n",
    "   test_size=0.3,\n",
    "   random_state=42,\n",
    "   stratify=df['department']  # Ensure balanced classes in both sets\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Class distribution in training set: \\n{y_train.value_counts()}\")\n",
    "print(f\"Class distribution in testing set: \\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29eab8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words features: 57\n",
      "TF-IDF features: 57\n",
      "Sample BoW features: ['access' 'account' 'account password' 'account show' 'add' 'address'\n",
      " 'app' 'billing' 'browser' 'change']\n",
      "Sample TF-IDF features (including bigrams): ['account password', 'account show', 'get error']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words vectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,  # Already done in preprocessing\n",
    "   min_df=2,  # Ignore terms that appear in fewer than 2 documents\n",
    "   max_df=0.95, # Ignore terms that appear in more than 95% of documents\n",
    "   ngram_range=(1, 2) # Include both single words and pairs of consecutive words\n",
    ")\n",
    "\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,\n",
    "   min_df=2,\n",
    "   max_df=0.95,\n",
    "   ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "\n",
    "# Apply vectorizers to training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Get feature information\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(f\"Bag of Words features: {len(count_features)}\")\n",
    "print(f\"TF-IDF features: {len(tfidf_features)}\")\n",
    "print(f\"Sample BoW features: {count_features[:10]}\")\n",
    "print(f\"Sample TF-IDF features (including bigrams): {[f for f in tfidf_features[:20] if ' ' in f][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9a1b0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ticket_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:175\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index_class_helper.pxi:70\u001b[39m, in \u001b[36mpandas._libs.index.Int64Engine._check_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'ticket_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Create document vectors for training and test sets\u001b[39;00m\n\u001b[32m     58\u001b[39m X_train_tokens = []\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m X_train[\u001b[33m'\u001b[39m\u001b[33mticket_text\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     60\u001b[39m    tokens = nltk.word_tokenize(text)\n\u001b[32m     61\u001b[39m    \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/pandas/core/series.py:1130\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_value(key)\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/pandas/core/series.py:1246\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m loc = \u001b[38;5;28mself\u001b[39m.index.get_loc(label)\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'ticket_text'"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model on our dataset\n",
    "# Note: In a real-world scenario, you'd use a much larger corpus\n",
    "# or pre-trained embeddings for better results\n",
    "w2v_model = Word2Vec(\n",
    "   df['tokens'],\n",
    "   vector_size=100,  # Dimension of the embedding vectors\n",
    "   window=5,  # Context window size\n",
    "   min_count=1,  # Ignore words with fewer occurrences\n",
    "   workers=4,  # Number of processors to use\n",
    "   sg=1  # Skip-gram model (1) instead of CBOW (0)\n",
    ")\n",
    "\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def document_to_vector(tokens, model, vector_size=100):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model.wv:\n",
    "           doc_vector += model.wv[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "def document_to_vector_pretrained(tokens, model, vector_size=300):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model:\n",
    "           doc_vector += model[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "# Create document vectors for training and test sets\n",
    "X_train_tokens = []\n",
    "for text in X_train['ticket_text']:\n",
    "   tokens = nltk.word_tokenize(text)\n",
    "   for token in tokens:\n",
    "      if token not in X_train_tokens:\n",
    "         X_train_tokens.append(token)\n",
    "X_test_tokens = []\n",
    "for text in X_test['ticket_text']:\n",
    "   tokens = nltk.word_tokenize(text)\n",
    "   for token in tokens:\n",
    "      if token not in X_train_tokens:\n",
    "         X_train_tokens.append(token)\n",
    "X_train_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "\n",
    "print(f\"Word2Vec document vectors shape: {X_train_w2v.shape}\")\n",
    "\n",
    "\n",
    "# Alternatively, download and use pre-trained embeddings\n",
    "# This takes more time but might give better results\n",
    "try:\n",
    "   # Attempt to download pre-trained embeddings (if internet is available)\n",
    "   pretrained_model = api.load('word2vec-google-news-300')\n",
    "   print(\"Pre-trained model loaded successfully.\")\n",
    "  \n",
    "   # Create vectors using pre-trained embeddings\n",
    "   X_train_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                   for tokens in X_train_tokens])\n",
    "   X_test_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                  for tokens in X_test_tokens])\n",
    "  \n",
    "   print(f\"Pre-trained document vectors shape: {X_train_pretrained.shape}\")\n",
    "   pretrained_available = True\n",
    "except Exception as e:\n",
    "   print(f\"Pre-trained embeddings could not be loaded: {e}\")\n",
    "   pretrained_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa80015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
